Model,MMLU,MMLU Citation,B Params
Llama 3.2 1B,49.30%,https://huggingface.co/meta-llama/Llama-3.2-1B#:~:text=Capability%20Benchmark%20,1
Apple OpenElm 1.1B,27.10%,https://huggingface.co/apple/OpenELM#:~:text=OpenELM,1.1
Llama 3.2 3B,63.40%,https://huggingface.co/meta-llama/Llama-3.2-1B#:~:text=Capability%20Benchmark%20,3
Mistral 7B,62.50%,https://developers.googleblog.com/en/introducing-gemma-models-in-keras/#:~:text=Gemma%20models%20come%20in%20portable,7
AppleOpenElm 3B,26.80%,https://huggingface.co/apple/OpenELM#:~:text=OpenELM,3
Llama 3.1 8B,69.40%,https://console.groq.com/docs/model/llama-3.1-8b-instant#:~:text=Performance%20Metrics,8
Qwen2.5-32b,83.30%,,32
Llama 3 8B,66.60%,https://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text=Base%20pretrained%20models,8
Gemma 8B,64.40%,https://predibase.com/blog/how-to-efficiently-fine-tune-gemma-7b-with-open-source-ludwig#:~:text=for%20their%20sizes%2C%20compared%20to,8
Gemma 2 9B,71.30%,https://console.groq.com/docs/model/gemma2-9b-it#:~:text=Performance%20Metrics,9
Llama-3.2-11B,73.00%,https://www.prompthackers.co/compare/llama-3.2-11b/llama-3.2-3b#:~:text=Discover%20how%20Meta%27s%20Llama%203,11
Llama 2 Chat 7B,45.80%,https://openlaboratory.ai/models/llama-2-7b#:~:text=Llama%202%207B%20exhibits%20enhanced,7
Llama 2 Chat 13B,53.60%,https://openlaboratory.ai/models/llama-2-13b#:~:text=Evaluation%20and%20Benchmark%20Performance,13
Gemma 2 27B,75.20%,https://openlaboratory.ai/models/gemma-2-9b#:~:text=Evaluations%20have%20placed%20Gemma%202,27
DBRX,74.70%,https://www.chaosgenius.io/blog/dbrx/#:~:text=1,36
Mixtral Large Base,71.90%,https://www.chaosgenius.io/blog/dbrx/#:~:text=1,39
Llama 3.1 70B,86.00%,https://blog.promptlayer.com/meta-model-analysis-llama-3-vs-3-1/#:~:text=Evaluation%20Category%20Llama%203,70
Qwen2.5 72B,86.10%,https://hub.researchgraph.org/qwen-2-5-is-it-really-that-good/#:~:text=In%20terms%20of%20benchmarking%2C%20Qwen2,72
Llama-3.1-Nemotron-70B-Instruct-HF,83.50%,https://huggingface.co/RedHatAI/Llama-3.1-Nemotron-70B-Instruct-HF-FP8-dynamic#:~:text=OpenLLM%20v1%20MMLU%20%285,70
Qwen2 72B,84.20%,https://huggingface.co/Qwen/Qwen2-72B#:~:text=Params%20236B%20140B%2070B%2072B,72
NVLM-D-72B,82.00%,https://huggingface.co/nvidia/NVLM-D-72B#:~:text=Gemini%201,72
Llama 3 70B,79.50%,https://huggingface.co/meta-llama/Meta-Llama-3-8B#:~:text=Base%20pretrained%20models,70
Claude 3 Sonnet,79.00%,,70
LLaMA2-70B Base,69.80%,https://github.com/meta-llama/llama-models/blob/main/models/llama2/MODEL_CARD.md#:~:text=Model%20Size%20Code%20Commonsense%20Reasoning,70
QwQ-32B,78.20%,,32
Gemma-3-27B,64.90%,,27
R1-32B,73.90%,,32
Deepseek-R1,90.80%,https://blog.promptlayer.com/deepseek-v3-vs-r1/#:~:text=DeepSeek%20R1%20focuses%20on%20logical,671
DeepSeek-v2,78.40%,,21
Deepseek-v3,87.10%,https://blog.promptlayer.com/deepseek-v3-vs-r1/#:~:text=Metric%20DeepSeek%20V3%20DeepSeek%20R1,37
Llama-3.2-90B,86.00%,https://ai.azure.com/catalog/models/Llama-3.2-90B-Vision-Instruct#:~:text=Text%20General%20MMLU%20,90
GPT-3.5,70.00%,https://www.chaosgenius.io/blog/dbrx/#:~:text=1,167
Grok-1,73.00%,https://www.chaosgenius.io/blog/dbrx/#:~:text=1,314
Nemotron-4-340B-Instruct,78.70%,,340
QWQ-32B,76.40%,,32
Qwen 3-235B,87.80%,,238
Qwen3-0.6B,52.81%,https://arxiv.org/abs/2505.09388,0.6
Qwen3-1.7B,62.63%,https://arxiv.org/abs/2505.09388,1.7
Qwen3-4B,69.50%,,4
Qwen3-8B,74.70%,,8
Qwen3-14B,78.50%,,14
Qwen3-32B,89.80%,,32
Llama 3 405B,85.20%,,405
Gemini Ultra,90.00%,https://galileo.ai/blog/mmlu-benchmark#:~:text=Current%20MMLU%20Leaderboard%20and%20Performance,1560
GPT-04-mini,81.60%,,1800
GPT-o3,82.90%,,1800
GPT-4o,88.70%,,1800
GPT-4,86.40%,https://galileo.ai/blog/mmlu-benchmark#:~:text=Current%20MMLU%20Leaderboard%20and%20Performance,1800
Claude 3 Opus,86.80%,,2000
Qwen3-235B,89.20%,,235
GOT-OSS-20B,73.60%,,20
GPT-OSS-120B,79.30%,,120
GPT‑5,,,
GPT‑5‑Mini,,,
GPT‑5‑Nano,,,
GPT‑5‑Codex,,,
Grok 4,,,
Llama 4 Scout,,,
Llama 4 Maverick,,,
Llama 4 Behemoth,,,
Gemini 2.5 Pro,,,
Gemini 2.5 Flash,,,
Gemini 2.5 Flash‑Lite,,,
Claude 4 Opus,,,
Claude 4 Sonnet,,,
Claude Opus 4.1,,,
Claude Sonnet 4.1,,,
Mistral Medium 3.1,,,
Magistral Medium 1.2,,,
MiniMax‑Text‑01,,,
MiniMax M1,,,
MiniMax M2,,,
Nova Premier,,,
Llama Nemotron Ultra,,,
Solar Pro 2,,,
Kimi K2,,,
Qwen 3 Max,,,
Qwen 3 VL Plus,,,
GLM 4.5,,,
